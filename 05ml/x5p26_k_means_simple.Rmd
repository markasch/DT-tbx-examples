---
title: "Simple example of k-means clustering"
author: "M. Asch"
date: "21/10/2019"
output:
  pdf_document: 
    keep_tex: yes
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The `kmeans`function of `R` implements the $k$-means clustering algorithm. We construct a simple, artifical example in order to bring out some of the basic points seen above.

Let us simulate a dataset with two clusters of Gaussian random variables, having a shift in their mean values.  

```{r}
set.seed(2)
x         <- matrix(rnorm(50*2), ncol=2)
x[1:25,1] <- x[1:25,1]+3
x[1:25,2] <- x[1:25,2]-4
head(x)
```

First, we perform k-means with $K=2$. 

```{r}
km.out=kmeans(x,2)
```

The affectations of the observations are in the variable `cluster`

```{r}
km.out$cluster
```

We observe that k-means has perfectly separated the observations into two clusters, without us having supplied any group information to `kmeans()`.

Let us plot the observations, each point colored according to its affectation.

```{r}
plot(x, col=(km.out$cluster+1), main="k-means clustering results with K=2", xlab="", ylab="", pch=20, cex=2)

```

The observations were easy to plot here, since we are in two dimensions only. In the case of more than two variables, we could perform a PCA, and then plot the two first principal components, for example.

In this simulated example, we knew the number of clusters. But in general, this is definitely not the case. So we could have started by trying $K=3$.

```{r}
set.seed(4)
km.out=kmeans(x,3,nstart=20)
km.out
plot(x, col=(km.out$cluster+1), main="k-means clustering results with K=3", xlab="", ylab="", pch=20, cex=2)
```

Here, the algorithm has divided one of the two clusters. Note that we have used the parameter `nstart=20` which instructs the algorithm to perform $20$ restarts with different, random initial clusters. To compare the performance of no restart, `nstart=1`, with 20 restarts, `nstart=20`, we extract the vectors of intra-cluster sum of squares, `km.out$withinss` and recover their totals in `km.out$tot.withinss`.

```{r}
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
```

The restarting has effectively reduced the value of the intra-cluster **total** sum of squares, and we have thus attained a better optimum (minimum). It is strongly recommended to use a high value of `nstart` whenever we perform k-means clustering. Values of 20 or 50 are commonly used. The `set.seed()` command is just for reproducibility.


